{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d8b7be-8a40-4685-a139-9ec02806d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "# For this example we will use pytorch to manage the construction of the neural networks and the training\n",
    "# torchvision is a module that is part of pytorch that supports vision datasets and it will be where we will source the mnist - handwritten digits - data\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d147959-ab1c-4707-a358-347a35093150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  9225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22f5552eb90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting a seed will determine which data elements are selected. To replicate results keep the same seed.\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c07a3b-8710-4279-b484-a896d49b32d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a check if there is a gpu available for training. At the moment we are assuming that it is not available.\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d66c27c-0856-40fd-875f-d5480adcb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the GPU is not available means we will set the device to cpu and set up some parameters\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cpu\")\n",
    "ngpu = 0\n",
    "#This is the width of the latent space matrix\n",
    "nz = 100\n",
    "# This is the generator matrix shape\n",
    "ngf = 64\n",
    "# This is the descrimator matrix shape\n",
    "ndf = 64\n",
    "# This is the number of color channels - other datasets may have 3 if they are color\n",
    "nc = 1\n",
    "# The number of sample to process per pass\n",
    "batch_size = 64\n",
    "# the number of CPU workers to work on the dataset\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7ed056d-5bbe-4492-bd09-068bbbcf9b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:11<00:00, 835198.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 261594.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:03<00:00, 512912.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dset.MNIST(root='data', download=True,   # Loads the MNIST dataset, which consists of handwritten digits, root='data': Specifies the directory where the dataset will be stored. download=True: Downloads the dataset if it's not already present in the specified directory.\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(64),  #Resizes the images to 64x64 pixels.\n",
    "                          transforms.ToTensor(),  #Converts the images to PyTorch tensors.\n",
    "                          transforms.Normalize((0.5,), (0.5,)),  #Normalizes the images with a mean of 0.5 and a standard deviation of 0.5.\n",
    "                      ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, #Creates a DataLoader to efficiently load data in batches,  The dataset to load (in this case, the MNIST dataset), The number of images to load in each batch.\n",
    "                                         shuffle=True, num_workers=int(workers)) #Randomly shuffles the data at every epoch, 4 subprocesses will be used to load the data in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f399933e-8cf6-4c0b-9b1e-7f0c11dd252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "# The weights will need to be initialised based on the layer type to some value before training. These could be imported from past training steps.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1: # Checks if the class name contains 'Conv' (indicating it's a convolutional layer)..\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02) #Initializes the weights of the convolutional layer with a normal distribution (mean=0.0, std=0.02)\n",
    "    elif classname.find('BatchNorm') != -1:  #checks if the class name contains 'BatchNorm' (indicating it's a batch normalization layer).\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02) #Initializes the weights of the batch normalization layer with a normal distribution (mean=1.0, std=0.02).\n",
    "        torch.nn.init.zeros_(m.bias)  #Initializes the biases of the batch normalization layer to zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ffec41e-7aad-4115-8fcd-546b2400b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This is the bulk of the neural network definition for the Generator.\n",
    "# The init sets up the layers and connecting activation functions.\n",
    "# The forward function processes the data through the layers\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu): #initializes the class, setting up the number of GPUs (ngpu) and calling the parent class's initializer.\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential( # is used to build a sequential container of layers.\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False), #layers perform the transpose convolution operations, which are essential for upsampling.\n",
    "            nn.BatchNorm2d(ngf * 8), #normalizes the outputs of the convolutional layers to stabilize and speed up the training.\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() #scales the output to the range [-1, 1].\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input): # defines how the data passes through the network.\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(  #parallelize the data across GPUs, If the input is on a CUDA device and multiple GPUs are available,\n",
    "                self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)  #the input data is processed through the network layers sequentially.\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "netG = Generator(ngpu).to(device)  #reates an instance of the Generator class and moves it to the specified device\n",
    "netG.apply(weights_init)  # initializes the weights of the network.\n",
    "print(netG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bca3b3ae-498a-42b7-a899-952f0f262c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# This is the bulk of the neural network definition for the Discrimator.\n",
    "# The init sets up the layers and connecting activation functions.\n",
    "# The forward function processes the data through the layers\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(\n",
    "                self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "    \n",
    "netD = Discriminator(ngpu).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "296f93ef-99df-4ad8-9713-fded8f4c0eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss function from pytorches established modules\n",
    "criterion = nn.BCELoss() #Binary Cross Entropy Loss function. It measures the error between the predicted output and the actual label (real or fake). \n",
    "#This loss function is commonly used in binary classification problems.\n",
    "\n",
    "# Set up the initial noise of the latent space to sample from.\n",
    "# Set the label of a real and fake sample to 0,1\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "#fixed_noise is a tensor of random values sampled from a normal distribution. It serves as the input to the generator to produce fake data. \n",
    "#The shape (64, nz, 1, 1) indicates 64 samples, each with nz dimensions, and the values are placed on the specified device (CPU or GPU).\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Create the optimiser which will dynamically change the parameters of the learning function over time to imporve the training process\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0005, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c68fad-0ffe-420f-9476-b0d960fd20f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the engine of the code base - explicitly taking the objects created above \n",
    "# (The generator, discrimator and the dataset) and connecting them together to learn.\n",
    "\n",
    "for epoch in range(1):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        \n",
    "        # Set the descrimator to forget any gradients:This line resets the gradients of the discriminator (netD). \n",
    "        #This is necessary to ensure that the gradients from the previous batch do not accumulate.\n",
    "        netD.zero_grad()\n",
    "        # Get a sample of real handwritten digits and label them as 1 - all real\n",
    "        real_cpu = data[0].to(device)  #etrieves a batch of real handwritten digits from the dataloader and moves it to the specified device (CPU or GPU).\n",
    "        batch_size = real_cpu.size(0)  #gets the number of samples in the current batch.\n",
    "        label = torch.full((batch_size,), real_label, dtype=real_cpu.dtype, device=device)  # creates a tensor of labels, all set to real_label (1), indicating that these samples are real.\n",
    "        # passes the real data through the discriminator to get its output.\n",
    "        output = netD(real_cpu)\n",
    "        # calculates the error (loss) between the discriminator's output and the real labels using the Binary Cross Entropy Loss function (criterion)\n",
    "        errD_real = criterion(output, label)\n",
    "        #computes the gradients of the error of each layer of the network with respect to the discriminator's parameters.\n",
    "        errD_real.backward()\n",
    "        # Get the average of the output across the batch of the discriminator for the real data, which can be used for monitoring the training process.\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake: Generates a batch of random noise vectors (noise) with the same batch size as the real data. \n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        # pass the noise through the generator layers (netG) to produce fake images (fake).\n",
    "\n",
    "        fake = netG(noise)\n",
    "        # Sets the labels for the fake data to fake_label (0), indicating that these samples are fake.\n",
    "        label.fill_(fake_label)\n",
    "        # ask the discrimator to judge the fake images\n",
    "        output = netD(fake.detach())\n",
    "        # measure the error\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients \n",
    "        errD_fake.backward()\n",
    "        # Get the average output across the batch again\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Get the error\n",
    "        errD = errD_real + errD_fake\n",
    "        # Run the optimizer to update the weights\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        # Set the gradients of the generator to zero\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # get the judgements from the discrimator of the generator output is fake\n",
    "        output = netD(fake)\n",
    "        # calculate the error\n",
    "        errG = criterion(output, label)\n",
    "        # update the gradients\n",
    "        errG.backward()\n",
    "        # Get the average of the output across the batch\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # update the weights\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, 1, i, len(dataloader), errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        # every 100 steps save a real sample and a fake sample for comparison\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,'real_samples.png',normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.detach(),'fake_samples_epoch_%03d.png' % epoch, normalize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
